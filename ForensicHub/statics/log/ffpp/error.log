[INFO] 2025-10-05 14:36:48,015 run: Running torch.distributed.run with args: ['/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/run.py', '--standalone', '--nnodes=1', '--nproc_per_node=2', '../training_scripts/train.py', '--config', '/data/disk2/yer/ForensicHub/ForensicHub/statics/bisai/focal_train_ffpp.yaml']
[INFO] 2025-10-05 14:36:48,017 run: 
**************************************
Rendezvous info:
--rdzv_backend=c10d --rdzv_endpoint=localhost:29400 --rdzv_id=dbb3c457-29f2-4366-83eb-e302718e8964
**************************************

[INFO] 2025-10-05 14:36:48,017 run: Using nproc_per_node=2.
[INFO] 2025-10-05 14:36:48,017 api: Starting elastic_operator with launch configs:
  entrypoint       : ../training_scripts/train.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : dbb3c457-29f2-4366-83eb-e302718e8964
  rdzv_backend     : c10d
  rdzv_endpoint    : localhost:29400
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

[INFO] 2025-10-05 14:36:48,018 c10d_rendezvous_backend: Process 38306 hosts the TCP store for the C10d rendezvous backend.
[INFO] 2025-10-05 14:36:48,020 local_elastic_agent: log directory set to: /tmp/torchelastic_msf9o0ms/dbb3c457-29f2-4366-83eb-e302718e8964_la1b1z6k
[INFO] 2025-10-05 14:36:48,020 api: [default] starting workers for entrypoint: python
[INFO] 2025-10-05 14:36:48,020 api: [default] Rendezvous'ing worker group
[INFO] 2025-10-05 14:36:48,020 dynamic_rendezvous: The node 'haida-KI4208G_38306_0' attempts to join the next round of the rendezvous 'dbb3c457-29f2-4366-83eb-e302718e8964'.
[INFO] 2025-10-05 14:36:48,048 dynamic_rendezvous: The node 'haida-KI4208G_38306_0' has joined round 0 of the rendezvous 'dbb3c457-29f2-4366-83eb-e302718e8964' as rank 0 in a world of size 1.
/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
[INFO] 2025-10-05 14:36:48,048 api: [default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=haida-KI4208G
  master_port=55343
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

[INFO] 2025-10-05 14:36:48,048 api: [default] Starting worker group
[INFO] 2025-10-05 14:36:48,049 __init__: Setting worker0 reply file to: /tmp/torchelastic_msf9o0ms/dbb3c457-29f2-4366-83eb-e302718e8964_la1b1z6k/attempt_0/0/error.json
[INFO] 2025-10-05 14:36:48,049 __init__: Setting worker1 reply file to: /tmp/torchelastic_msf9o0ms/dbb3c457-29f2-4366-83eb-e302718e8964_la1b1z6k/attempt_0/1/error.json
/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/IMDLBenCo/training_scripts/trainer/trainer.py:25: UserWarning: HOPE YOU TO READ: Starting from IMDLBenCo version v0.1.31, we have discovered a misdefinition regarding the --if_not_amp parameter in the argument parser. For more details, please refer to: https://github.com/scu-zjz/IMDLBenCo/issues/89. As a result, if you use the train.py script generated by an earlier version of 'benco init' before v0.1.30, along with the latest version of trainer.py, it may lead to AMP (Automatic Mixed Precision) being incorrectly enabled or disabled. We recommend updating to the latest version(after v0.1.31, include) of IMDLBenCo and re-initializing the project using 'benco init' to ensure that the --if_not_amp parameter is correctly defined.  Or for already initialized projects, please manually modify the action of the --if_not_amp parameter in train.py to 'store_true' to ensure correct logic.  We apologize for any inconvenience this may have caused.!!!!!!
  warnings.warn(
/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/IMDLBenCo/training_scripts/trainer/trainer.py:25: UserWarning: HOPE YOU TO READ: Starting from IMDLBenCo version v0.1.31, we have discovered a misdefinition regarding the --if_not_amp parameter in the argument parser. For more details, please refer to: https://github.com/scu-zjz/IMDLBenCo/issues/89. As a result, if you use the train.py script generated by an earlier version of 'benco init' before v0.1.30, along with the latest version of trainer.py, it may lead to AMP (Automatic Mixed Precision) being incorrectly enabled or disabled. We recommend updating to the latest version(after v0.1.31, include) of IMDLBenCo and re-initializing the project using 'benco init' to ensure that the --if_not_amp parameter is correctly defined.  Or for already initialized projects, please manually modify the action of the --if_not_amp parameter in train.py to 'store_true' to ensure correct logic.  We apologize for any inconvenience this may have caused.!!!!!!
  warnings.warn(
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/dataset/ffpp_dataset.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mask = torch.tensor(mask_np, dtype=torch.long).unsqueeze(0)
Traceback (most recent call last):
  File "../training_scripts/train.py", line 278, in <module>
    main(args, model_args, train_dataset_args, test_dataset_args, transform_args, evaluator_args)
  File "../training_scripts/train.py", line 193, in main
    train_stats = train_one_epoch(
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/IMDLBenCo/training_scripts/trainer/trainer.py", line 84, in train_one_epoch
    output_dict = model(**data_dict, 
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/models/focal_for_bisai/focal_for_bisai.py", line 111, in forward
    feat = self.backbone(image)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/disk2/yer/ForensicHub/ForensicHub/tasks/bisai/models/focal_for_bisai/focal_for_bisai.py", line 94, in forward
    y = self.enc(x)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/disk2/yer/ForensicHub/external/FOCAL/models/hrnet.py", line 563, in forward
    return self.net(x)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/disk2/yer/ForensicHub/external/FOCAL/models/hrnet.py", line 511, in forward
    feats = self.stage4(x_list)[0]  # B, 32, H // 4, W // 4
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/disk2/yer/ForensicHub/external/FOCAL/models/hrnet.py", line 258, in forward
    y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 697, in forward
    self.num_batches_tracked = self.num_batches_tracked + 1
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1176, in __setattr__
    buffers[name] = value
KeyboardInterrupt
[INFO] 2025-10-05 14:42:33,583 dynamic_rendezvous: The node 'haida-KI4208G_38306_0' has closed the rendezvous 'dbb3c457-29f2-4366-83eb-e302718e8964'.
Traceback (most recent call last):
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/run.py", line 637, in <module>
    main()
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/run.py", line 629, in main
    run(args)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 238, in launch_agent
    result = agent.run()
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 700, in run
    result = self._invoke_run(role)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 828, in _invoke_run
    time.sleep(monitor_interval)
KeyboardInterrupt
