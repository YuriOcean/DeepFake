[INFO] 2025-10-05 20:25:04,165 run: Running torch.distributed.run with args: ['/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/run.py', '--standalone', '--nnodes=1', '--nproc_per_node=2', '../training_scripts/train.py', '--config', '/data/disk2/yer/ForensicHub/ForensicHub/statics/bisai/focal_train_columbia.yaml']
[INFO] 2025-10-05 20:25:04,166 run: 
**************************************
Rendezvous info:
--rdzv_backend=c10d --rdzv_endpoint=localhost:29400 --rdzv_id=c06cf68d-f90c-4892-aebf-96f90ee4d449
**************************************

[INFO] 2025-10-05 20:25:04,167 run: Using nproc_per_node=2.
[INFO] 2025-10-05 20:25:04,167 api: Starting elastic_operator with launch configs:
  entrypoint       : ../training_scripts/train.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : c06cf68d-f90c-4892-aebf-96f90ee4d449
  rdzv_backend     : c10d
  rdzv_endpoint    : localhost:29400
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

[INFO] 2025-10-05 20:25:04,168 c10d_rendezvous_backend: Process 32198 hosts the TCP store for the C10d rendezvous backend.
[INFO] 2025-10-05 20:25:04,170 local_elastic_agent: log directory set to: /tmp/torchelastic_hr89nl3g/c06cf68d-f90c-4892-aebf-96f90ee4d449_oe0d07qh
[INFO] 2025-10-05 20:25:04,170 api: [default] starting workers for entrypoint: python
[INFO] 2025-10-05 20:25:04,170 api: [default] Rendezvous'ing worker group
[INFO] 2025-10-05 20:25:04,170 dynamic_rendezvous: The node 'haida-KI4208G_32198_0' attempts to join the next round of the rendezvous 'c06cf68d-f90c-4892-aebf-96f90ee4d449'.
[INFO] 2025-10-05 20:25:04,400 dynamic_rendezvous: The node 'haida-KI4208G_32198_0' has joined round 0 of the rendezvous 'c06cf68d-f90c-4892-aebf-96f90ee4d449' as rank 0 in a world of size 1.
/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
[INFO] 2025-10-05 20:25:04,401 api: [default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=haida-KI4208G
  master_port=40083
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

[INFO] 2025-10-05 20:25:04,401 api: [default] Starting worker group
[INFO] 2025-10-05 20:25:04,402 __init__: Setting worker0 reply file to: /tmp/torchelastic_hr89nl3g/c06cf68d-f90c-4892-aebf-96f90ee4d449_oe0d07qh/attempt_0/0/error.json
[INFO] 2025-10-05 20:25:04,402 __init__: Setting worker1 reply file to: /tmp/torchelastic_hr89nl3g/c06cf68d-f90c-4892-aebf-96f90ee4d449_oe0d07qh/attempt_0/1/error.json
/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/IMDLBenCo/training_scripts/trainer/trainer.py:25: UserWarning: HOPE YOU TO READ: Starting from IMDLBenCo version v0.1.31, we have discovered a misdefinition regarding the --if_not_amp parameter in the argument parser. For more details, please refer to: https://github.com/scu-zjz/IMDLBenCo/issues/89. As a result, if you use the train.py script generated by an earlier version of 'benco init' before v0.1.30, along with the latest version of trainer.py, it may lead to AMP (Automatic Mixed Precision) being incorrectly enabled or disabled. We recommend updating to the latest version(after v0.1.31, include) of IMDLBenCo and re-initializing the project using 'benco init' to ensure that the --if_not_amp parameter is correctly defined.  Or for already initialized projects, please manually modify the action of the --if_not_amp parameter in train.py to 'store_true' to ensure correct logic.  We apologize for any inconvenience this may have caused.!!!!!!
  warnings.warn(
/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/IMDLBenCo/training_scripts/trainer/trainer.py:25: UserWarning: HOPE YOU TO READ: Starting from IMDLBenCo version v0.1.31, we have discovered a misdefinition regarding the --if_not_amp parameter in the argument parser. For more details, please refer to: https://github.com/scu-zjz/IMDLBenCo/issues/89. As a result, if you use the train.py script generated by an earlier version of 'benco init' before v0.1.30, along with the latest version of trainer.py, it may lead to AMP (Automatic Mixed Precision) being incorrectly enabled or disabled. We recommend updating to the latest version(after v0.1.31, include) of IMDLBenCo and re-initializing the project using 'benco init' to ensure that the --if_not_amp parameter is correctly defined.  Or for already initialized projects, please manually modify the action of the --if_not_amp parameter in train.py to 'store_true' to ensure correct logic.  We apologize for any inconvenience this may have caused.!!!!!!
  warnings.warn(
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[INFO] 2025-10-05 20:25:30,360 dynamic_rendezvous: The node 'haida-KI4208G_32198_0' has closed the rendezvous 'c06cf68d-f90c-4892-aebf-96f90ee4d449'.
Traceback (most recent call last):
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/run.py", line 637, in <module>
    main()
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/run.py", line 629, in main
    run(args)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 238, in launch_agent
    result = agent.run()
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 700, in run
    result = self._invoke_run(role)
  File "/var/tmp/yer/anaconda3/envs/rethinking/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 828, in _invoke_run
    time.sleep(monitor_interval)
KeyboardInterrupt
